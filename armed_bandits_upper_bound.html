
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>A class of algorithms: index policies &#8212; Reinforcement learning</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Reinforcement learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="content.html">
   Content in Jupyter Book
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="markdown.html">
     Markdown Files
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="notebooks.html">
     Content with notebooks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/armed_bandits_upper_bound.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Farmed_bandits_upper_bound.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   A class of algorithms: index policies
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-the-choice-of-c-i-t">
   Why the choice of
   <span class="math notranslate nohighlight">
    \(c(i,t)\)
   </span>
   ?
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proof-of-the-logarithmic-regret">
   Proof of the logarithmic regret
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      R: "{\\mathbb{R}}",
      Q: "{\\mathbb{Q}}",
      N: "{\\mathbb{N}}",
      Z: "{\\mathbb{Z}}",
      A: "{\\mathcal{A}}",
      B: "{\\mathcal{B}}",
      E: "{\\mathbb{E}}",
      P: "{\\mathbb{P}}",
    }
  }
});
</script>
<p class="intro"><span class="dropcap">W</span>e present a proof that the upper confidence bound yields an (asymptotically) optimal algorithm for regret minimisation of multi-armed bandits.</p>
<p>There is an accompanying github repository for experimenting with different algorithms for the <a class="reference external" href="https://github.com/nathanael-fijalkow/Multi-Armed-Bandits">multi-armed bandits</a>.</p>
<p>This result was proved by Auer, Cesa-Bianchi, and Fischer in <a class="reference external" href="http://homes.di.unimi.it/cesa-bianchi/Pubblicazioni/ml-02.pdf">this paper</a>.
The proof is essentially the same (some calculations are a bit different).</p>
<p>The setting is the following: there are <span class="math notranslate nohighlight">\(K\)</span> machines, which are each given by a distribution on rewards in <span class="math notranslate nohighlight">\([0,1]\)</span>.
Initially, we do not know anything except for the number <span class="math notranslate nohighlight">\(K\)</span> of machines.
At each time step, we can choose a machine <span class="math notranslate nohighlight">\(i\)</span> and get a reward from <span class="math notranslate nohighlight">\(i\)</span> drawn from the distribution attached to this machine.
The goal is to <strong>maximise the expected total payoff</strong>, as a function of the number of steps.
An algorithm selects at each time step a machine <span class="math notranslate nohighlight">\(i\)</span> based on the choices made and rewards observed so far.
We write (for the sake of clarity we will not mark the dependence on the algorithm on each random variables):</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(i\)</span> is typically a machine (in <span class="math notranslate nohighlight">\([1,K]\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu_i\)</span> is the expectation of the distribution for the machine <span class="math notranslate nohighlight">\(i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(t,T\)</span> are number of steps</p></li>
<li><p><span class="math notranslate nohighlight">\(n(i,T)\)</span> is the number of times the machine <span class="math notranslate nohighlight">\(i\)</span> was played in the first <span class="math notranslate nohighlight">\(T\)</span> steps</p></li>
<li><p><span class="math notranslate nohighlight">\(M(t)\)</span> is the machine chosen at time step <span class="math notranslate nohighlight">\(t\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(X(t)\)</span> is the reward from the machine chosen at time step <span class="math notranslate nohighlight">\(t\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(X(i,t)\)</span> is the reward of playing the machine <span class="math notranslate nohighlight">\(i\)</span> at time step <span class="math notranslate nohighlight">\(t\)</span>; if <span class="math notranslate nohighlight">\(i\)</span> was not chosen at time <span class="math notranslate nohighlight">\(t\)</span>, then it is <span class="math notranslate nohighlight">\(0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\widehat{X}(i,T)\)</span> is the <strong>empirical</strong> reward of machine <span class="math notranslate nohighlight">\(i\)</span> in the first <span class="math notranslate nohighlight">\(T\)</span> steps, defined by</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\widehat{X}(i,T) = \frac{1}{n(i,T)} \sum_{t \in [1,T]} X(i,t)\]</div>
<p>An optimal machine is one maximising the average reward <span class="math notranslate nohighlight">\(\mu\)</span>.
We write $<span class="math notranslate nohighlight">\(*\)</span><span class="math notranslate nohighlight">\( for this machine, so for instance its average reward is \)</span><span class="math notranslate nohighlight">\(\mu_{*}\)</span>$.</p>
<p>The goal is to maximise</p>
<div class="math notranslate nohighlight">
\[
\E [ \sum_{t \in [1,T]} X(t) ]
\]</div>
<p>Equivalently, we can define the regret of an algorithm
as the difference between the optimal reward in <span class="math notranslate nohighlight">\(T\)</span> steps (which is simply <span class="math notranslate nohighlight">\(\mu_* \cdot T\)</span>) and the obtained reward:</p>
<div class="math notranslate nohighlight">
\[
R = \mu_* \cdot T - \sum_{t \in [1,T]} X(t)
\]</div>
<p>Maximising the expected total payoff is the same as minimising the regret.
Note that both are functions of <span class="math notranslate nohighlight">\(T\)</span> the total number of steps, hence it is not clear what it means to minimise or maximise.
We may consider the asymptotic behaviour, but of course it is more desirable to also give bounds for any <span class="math notranslate nohighlight">\(T\)</span>.</p>
<div class="section" id="a-class-of-algorithms-index-policies">
<h1>A class of algorithms: index policies<a class="headerlink" href="#a-class-of-algorithms-index-policies" title="Permalink to this headline">¶</a></h1>
<p>A very natural class of algorithms is to assign at each time step <span class="math notranslate nohighlight">\(t\)</span> and to each machine <span class="math notranslate nohighlight">\(i\)</span> a number <span class="math notranslate nohighlight">\(U(i,t)\)</span>,
and to choose at each step the machine maximising this number.
(We first choose each machine once, to meaningfully initialise these numbers. We will not take this phase into account in the analysis later.)</p>
<p>The first naive attempt is to use <span class="math notranslate nohighlight">\(U(i,t) = \widehat{X}(i,t)\)</span>.
The empirical reward converges towards the actual expected reward, i.e. <span class="math notranslate nohighlight">\(\lim_T \widehat{X}(i,T) = \mu_i\)</span>,
but this does not imply that this algorithm finds the optimal machine in the limit.
Indeed, it may choose a machine once, get a bad reward from it, and decide never to play it again.
Consider for instance the case of two machines, the first one giving always reward <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span>, and the second giving reward <span class="math notranslate nohighlight">\(0\)</span> with probability <span class="math notranslate nohighlight">\(\frac{1}{4}\)</span>
and <span class="math notranslate nohighlight">\(1\)</span> with probability <span class="math notranslate nohighlight">\(\frac{3}{4}\)</span>. The second machine is optimal since <span class="math notranslate nohighlight">\(\mu_1 = \frac{1}{2}\)</span> and <span class="math notranslate nohighlight">\(\mu_2 = \frac{3}{4}\)</span>, but if we are unlucky and the first reward given by the machine <span class="math notranslate nohighlight">\(2\)</span> is <span class="math notranslate nohighlight">\(0\)</span>,
then the machine <span class="math notranslate nohighlight">\(2\)</span> will never be used again. This yields a regret which is linear in <span class="math notranslate nohighlight">\(T\)</span>.</p>
<p>The missing information is whether the empirical reward for machine <span class="math notranslate nohighlight">\(i\)</span> can be considered <strong>accurate or not</strong>, in other words whether the machine <span class="math notranslate nohighlight">\(i\)</span> was played enough times to trust the empirical reward.
This leads to indices of the form <span class="math notranslate nohighlight">\(U(i,t) = \widehat{X}(i,t) + c(i,t)\)</span>, with <span class="math notranslate nohighlight">\(c(i,t)\)</span> reflecting on the quality of the empirical reward.</p>
<p>Another point of view is that using <span class="math notranslate nohighlight">\(U(i,t) = \widehat{X}(i,t)\)</span> focusses on <strong>exploitation</strong> and does not reward <strong>exploration</strong>, which is precisely what <span class="math notranslate nohighlight">\(c(i,t)\)</span> does.</p>
<p>It turns out that there is a choice of <span class="math notranslate nohighlight">\(c(i,t)\)</span> which yields an algorithm with logarithmic regret:</p>
<blockquote>
<div><p><strong>Theorem:</strong> (Auer, Cesa-Bianchi, and Fischer 2002)
The regret of the index policy for <span class="math notranslate nohighlight">\(c(i,t) = \sqrt{ \frac{\log(t)}{n(i,t)} }\)</span> is <span class="math notranslate nohighlight">\(O(\log(T))\)</span></p>
</div></blockquote>
<p>This is asymptotically optimal as proved by Lai and Robbins in 1985.</p>
</div>
<div class="section" id="why-the-choice-of-c-i-t">
<h1>Why the choice of <span class="math notranslate nohighlight">\(c(i,t)\)</span>?<a class="headerlink" href="#why-the-choice-of-c-i-t" title="Permalink to this headline">¶</a></h1>
<p>The role of <span class="math notranslate nohighlight">\(c(i,t)\)</span> is to quantify the accuracy of the empirical reward.
Indeed, if the machine <span class="math notranslate nohighlight">\(i\)</span> was not played a lot of times, its emprical reward may be off by a lot,
and it is therefore interesting to choose it to increase the accuracy of the empirical reward, so <span class="math notranslate nohighlight">\(c(i,t)\)</span> should be large in that case.</p>
<p>The Chernoff-Hoeffding bound gives some information on the accuracy of the empirical reward:</p>
<div class="math notranslate nohighlight">
\[
\P(\widehat{X}(i,t) \ge \mu_i + c) \le \exp^{-2 c^2 n(i,t)}
\]</div>
<p>and similarly</p>
<div class="math notranslate nohighlight">
\[
\P(\widehat{X}(i,t) \le \mu_i - c) \le \exp^{-2 c^2 n(i,t)}
\]</div>
<p>So, choosing <span class="math notranslate nohighlight">\(c(i,t) = \sqrt{ \frac{\log(t)}{n(i,t)} }\)</span> implies an upper bound in <span class="math notranslate nohighlight">\(t^{-2}\)</span>, which we will see is exactly what we need.</p>
</div>
<div class="section" id="proof-of-the-logarithmic-regret">
<h1>Proof of the logarithmic regret<a class="headerlink" href="#proof-of-the-logarithmic-regret" title="Permalink to this headline">¶</a></h1>
<p>We first observe that the regret <span class="math notranslate nohighlight">\(R\)</span> is equal to</p>
<div class="math notranslate nohighlight">
\[
\sum_{i \in [1,K]} (\mu_* - \mu_i) \cdot n(i,T)
\]</div>
<p>Indeed, <span class="math notranslate nohighlight">\(\E[\sum_{t \in [1,T]} X_t] = \sum_{i \in [1,K]} \mu_i \cdot n(i,T)\)</span>.</p>
<p>This means that it will be enough to upper bound <span class="math notranslate nohighlight">\(n(i,T)\)</span> for all non-optimal machines <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>We let <span class="math notranslate nohighlight">\(\Delta_i\)</span> denote <span class="math notranslate nohighlight">\(\mu_* - \mu_i\)</span>.</p>
<p>We fix a non-optimal machine <span class="math notranslate nohighlight">\(i\)</span>.
Let <span class="math notranslate nohighlight">\(\ell\)</span> to be determined later: since when the machine <span class="math notranslate nohighlight">\(i\)</span> has been played only a few times, the empirical reward is very noisy,
we will be interested in studying the situation where it has been played at least <span class="math notranslate nohighlight">\(\ell\)</span> times.
For an event <span class="math notranslate nohighlight">\(A\)</span>, we write $<span class="math notranslate nohighlight">\(\{A\}\)</span><span class="math notranslate nohighlight">\( for the random variable with value \)</span>1<span class="math notranslate nohighlight">\( if \)</span>A<span class="math notranslate nohighlight">\( is realised and \)</span>0$ otherwise.</p>
<div class="math notranslate nohighlight">
\[
n(i,T) \le \ell + \sum_{t \in [1,T]} \{M(t) = i \wedge n(i,t) \ge \ell\}
\]</div>
<p>By definition of the algorithm, for the machine <span class="math notranslate nohighlight">\(i\)</span> to be chosen over the optimal machine <span class="math notranslate nohighlight">\(*\)</span>, we must have</p>
<div class="math notranslate nohighlight">
\[
\widehat{X}_{*,t} + c(*,t) \le \widehat{X}_{i,t} + c(i,t)
\]</div>
<p>Let us denote by <span class="math notranslate nohighlight">\(A\)</span> the event</p>
<div class="math notranslate nohighlight">
\[
\widehat{X}_{*,t} + c(*,t) \le \widehat{X}_{i,t} + c(i,t)\ \wedge\ n(i,t) \ge \ell
\]</div>
<p>Let us define two events:</p>
<ul>
<li><div class="math notranslate nohighlight">
\[A_1 = \widehat{X}_{*,t} \le \mu_{*} - c(*,t)$$, meaning that the optimal machine $*$ is underapproximated by $$c(*,t)\]</div>
</li>
<li><div class="math notranslate nohighlight">
\[A_2 = \widehat{X}_{i,t} \ge \mu_i + c(i,t)$$, meaning that $i$ is overapproximated by $$c(i,t)\]</div>
</li>
</ul>
<p>We let <span class="math notranslate nohighlight">\(\ell = \frac{4 \log(T)}{\Delta_i^2}\)</span>, and show that this implies that
$<span class="math notranslate nohighlight">\(A \subseteq A_1 \vee A_2.\)</span>$</p>
<p>First, we have
$<span class="math notranslate nohighlight">\(\mu_* \ge \mu_i + 2 c(i,t).\)</span><span class="math notranslate nohighlight">\(
Indeed, for \)</span>n(i,t) \ge \ell<span class="math notranslate nohighlight">\(, we have
\)</span><span class="math notranslate nohighlight">\(2c(i,t) = \sqrt{\frac{4 \log(t)}{n(i,t)}} \le \Delta_i = \mu_* - \mu_i.\)</span>$</p>
<p>We prove the contrapositive. Assume that neither $<span class="math notranslate nohighlight">\(A_1\)</span><span class="math notranslate nohighlight">\( nor \)</span><span class="math notranslate nohighlight">\(A_2\)</span><span class="math notranslate nohighlight">\( are realised, then we have
\)</span><span class="math notranslate nohighlight">\(\widehat{X}_{*,t} + c(*,t) &gt; \mu_{*} \ge \mu_i + 2 c(i,t) &gt; \widehat{X}_{i,t} + c(i,t),\)</span><span class="math notranslate nohighlight">\(
meaning that \)</span><span class="math notranslate nohighlight">\(A\)</span>$ is not realised.</p>
<p>It follows that the probability of <span class="math notranslate nohighlight">\(A\)</span> is bounded from above by the sum of the probabilities of the two events $<span class="math notranslate nohighlight">\(A_1\)</span><span class="math notranslate nohighlight">\( and \)</span><span class="math notranslate nohighlight">\(A_2\)</span><span class="math notranslate nohighlight">\(.
The discussion above for the Chernoff-Hoeffding bound shows that they have each probability upper bounded by \)</span>t^{-2}$.</p>
<p>Thus
$<span class="math notranslate nohighlight">\(
n(i,T) \le \ell + \sum_{t \ge 1} 2 t^{-2} \le \frac{4 \log(T)}{\Delta_i^2} + \frac{\pi^2}{3}
\)</span>$</p>
<p>We can conclude: the regret is bounded by</p>
<div class="math notranslate nohighlight">
\[
\left( \sum_{i \in [1,K]} \frac{4}{\Delta_i^2} \right) \cdot \log(T) + \frac{\pi^2}{3} = O(\log(T))
\]</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Nathanaël Fijalkow, Guillaume Lagarde, and Théo Matricon<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>