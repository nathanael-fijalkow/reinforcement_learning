
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Model &#8212; Reinforcement learning</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"R": "\\mathbb{R}", "Q": "\\mathbb{Q}", "N": "\\mathbb{N}", "Z": "\\mathbb{Z}", "M": "\\mathcal{M}", "A": "\\mathcal{A}", "B": "\\mathcal{B}", "E": "\\mathbb{E}", "P": "\\mathbb{P}", "val": "\\text{val}", "Dist": "\\mathcal{D}"}}, "tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Two-player games" href="../TwoPlayer/index.html" />
    <link rel="prev" title="The Monte Carlo approach" href="index.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Reinforcement learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Multi-armed bandits
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../MAB/index.html">
   Multi-armed bandits
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../MAB/intro.html">
     The model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MAB/proof_UCB.html">
     Proof of the UCB algorithm
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Markov decision processes and dynamic algorithms
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../MDP/index.html">
   Markov decision processes
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../MDP/intro.html">
     Model
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../DynamicAlgorithms/index.html">
   Dynamic algorithms
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../DynamicAlgorithms/intro.html">
     Dynamic algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../DynamicAlgorithms/finite_horizon.html">
     The finite horizon case
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../DynamicAlgorithms/discounted.html">
     The discounted reward
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../DynamicAlgorithms/Bellman_equations.html">
     Bellman equations: characterisation of the optimal values
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../DynamicAlgorithms/value_iteration.html">
     The value iteration algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../DynamicAlgorithms/policy_iteration.html">
     The policy iteration algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../DynamicAlgorithms/generalised_policy_iteration.html">
     The generalised policy iteration algorithm
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Monte Carlo approaches, temporal differences, and off-policy learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="index.html">
   The Monte Carlo approach
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Model
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Two-player games
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../TwoPlayer/index.html">
   Two-player games
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../TwoPlayer/intro.html">
     The model of two-player games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../TwoPlayer/classic.html">
     Classical algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../TwoPlayer/MCTS.html">
     Monte Carlo Tree Search (MCTS) for two-player games
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/MonteCarlo/intro.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://nathanael-fijalkow.github.com/reinforcement_learning"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://nathanael-fijalkow.github.com/reinforcement_learning/issues/new?title=Issue%20on%20page%20%2FMonteCarlo/intro.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-monte-carlo-approach">
   The Monte Carlo approach
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-evaluation-task">
   The evaluation task
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-improvement-task">
   The improvement task
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-complete-algorithm">
   The complete algorithm
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="model">
<h1>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h1>
<p>We consider the setting where the MDP is only known through simulation and show how to adapt the previous algorithms using statistics instead of exact computations.</p>
<p>We now lift the two (unrealistic) assumptions made by dynamic algorithms:</p>
<ul class="simple">
<li><p>the MDP has a finite set of states and actions, which can be both tabularised</p></li>
<li><p>the transition function is known</p></li>
</ul>
<p>The algorithmic setting is as follows.
There is an underlying MDP dictating the dynamics, but the controller does not know it (initially), he acquires knowledge of the MDP through interaction.</p>
<p>At each time step the controller knows in which state is the MDP and the set of actions available at this state.
(In our original definition, the set of actions is the same for every states, but sometimes it is useful to refine this and have different sets of actions for each state.)
This means that states can be finitely represented, but we do not assume that there are finitely many, and more importantly we do not assume that we know what is the set of all states.
From a state the controller triggers an action, which results in a reward and moving to the next state.
The controller <strong>does not know the transition distribution</strong>: it is sampled each time the controller triggers an action, which means that the controller can make statistics, but he never knows exact probabilities.</p>
<div class="section" id="the-monte-carlo-approach">
<h2>The Monte Carlo approach<a class="headerlink" href="#the-monte-carlo-approach" title="Permalink to this headline">¶</a></h2>
<p>The dynamic algorithms we constructed in the first part are based on solving two tasks:</p>
<ul class="simple">
<li><p><strong>evaluation</strong>: given a strategy $<span class="math notranslate nohighlight">\(\sigma\)</span><span class="math notranslate nohighlight">\(, compute (or approximate) \)</span><span class="math notranslate nohighlight">\(\val_\sigma\)</span>$</p></li>
<li><p><strong>improvement</strong>: given a strategy $<span class="math notranslate nohighlight">\(\sigma\)</span><span class="math notranslate nohighlight">\( and its (approximate) value function \)</span><span class="math notranslate nohighlight">\(\val_\sigma\)</span><span class="math notranslate nohighlight">\(, construct an improved strategy \)</span><span class="math notranslate nohighlight">\(\sigma'\)</span>$</p></li>
</ul>
<p>We will keep the general architecture of the algorithms, while adapting the solution of the two tasks to our statistical setting.</p>
<p>In the dynamic algorithms we were only manipulating pure positional strategies.
Here we will see that it will be useful to allow randomised strategies.</p>
<p>The Monte Carlo approach makes one key assumption:</p>
<ul class="simple">
<li><p><strong>finite episodes</strong>: every play terminates whatever the choice of actions, which means reaches a state called terminal, where there are no further actions.
This is reasonable for most games, where a winner is eventually determined, in finite time.
This assumption also means that whenever a terminal state is reached we can start over from the initial state again to produce a new episode.</p></li>
</ul>
</div>
<div class="section" id="the-evaluation-task">
<h2>The evaluation task<a class="headerlink" href="#the-evaluation-task" title="Permalink to this headline">¶</a></h2>
<p>Recall that the dynamic solution for the evaluation task was based on the following equations for a strategy $<span class="math notranslate nohighlight">\(\sigma:\)</span>$</p>
<div class="math notranslate nohighlight">
\[
\val_\sigma(s) = q_\sigma(s,\sigma(s)) = \sum_{s',r} \Delta(s,a)(s',r) (r + \gamma \val_\sigma(s')).
\]</div>
<p>The problem is that now we do not assume that we know $<span class="math notranslate nohighlight">\(\Delta(s,a),\)</span>$ so this quantity is not easily computed.</p>
<p>The most naive and yet effective solution is simply to sample plays according to $<span class="math notranslate nohighlight">\(\sigma\)</span><span class="math notranslate nohighlight">\(.
Starting from the initial state \)</span><span class="math notranslate nohighlight">\(s_0,\)</span><span class="math notranslate nohighlight">\( choose the action prescribed by \)</span><span class="math notranslate nohighlight">\(\sigma\)</span><span class="math notranslate nohighlight">\( and continue until reaching a terminal state.
Repeat this operation a large number of times, say \)</span><span class="math notranslate nohighlight">\(N\)</span><span class="math notranslate nohighlight">\(, leading to a sample of \)</span><span class="math notranslate nohighlight">\(N\)</span><span class="math notranslate nohighlight">\( plays.
For each state \)</span><span class="math notranslate nohighlight">\(s\)</span><span class="math notranslate nohighlight">\(, for each play containing \)</span><span class="math notranslate nohighlight">\(s\)</span><span class="math notranslate nohighlight">\( compute the total reward **from the first visit** of \)</span><span class="math notranslate nohighlight">\(s\)</span><span class="math notranslate nohighlight">\( in that play,
and set \)</span><span class="math notranslate nohighlight">\(\widehat{\val}_\sigma(s)\)</span><span class="math notranslate nohighlight">\( to be the average of these total rewards.
The hat over \)</span><span class="math notranslate nohighlight">\(\val\)</span><span class="math notranslate nohighlight">\( signifies that the value is **empirical**.
Using the equation above \)</span><span class="math notranslate nohighlight">\(\widehat{\val}_\sigma(s)\)</span><span class="math notranslate nohighlight">\( induces \)</span><span class="math notranslate nohighlight">\(\widehat{q}_\sigma.\)</span>$</p>
<p>Note that the only design choice so far in the naive Monte Carlo approach was whether we consider each visit or only the first in a play.
Both would lead to converging algorithms, as long as we evaluate randomised positional strategies.</p>
</div>
<div class="section" id="the-improvement-task">
<h2>The improvement task<a class="headerlink" href="#the-improvement-task" title="Permalink to this headline">¶</a></h2>
<p>Recall that given a strategy $<span class="math notranslate nohighlight">\(\sigma\)</span><span class="math notranslate nohighlight">\( and its value function \)</span><span class="math notranslate nohighlight">\(\val_\sigma\)</span>$,
we define</p>
<div class="math notranslate nohighlight">
\[
\sigma'(s) = \text{argmax}_{a \in A} q_{\sigma}(s,a)
\]</div>
<p>The problem here is that since we will be evaluating $<span class="math notranslate nohighlight">\(\sigma\)</span><span class="math notranslate nohighlight">\( using statistics on the plays of \)</span><span class="math notranslate nohighlight">\(\sigma\)</span><span class="math notranslate nohighlight">\(, 
we do not have any information about the outcomes of the actions that are not played by \)</span><span class="math notranslate nohighlight">\(\sigma\)</span><span class="math notranslate nohighlight">\(. 
In other words, if \)</span><span class="math notranslate nohighlight">\(\sigma\)</span><span class="math notranslate nohighlight">\( does not maintain **exploration**, then our value function \)</span><span class="math notranslate nohighlight">\(\val_\sigma\)</span><span class="math notranslate nohighlight">\( will not give us information outside of the behaviour of \)</span><span class="math notranslate nohighlight">\(\sigma\)</span>$,
hence not allow us to try interesting new moves.</p>
<p>There are different ways to ensure <strong>exploration</strong>, a prominent one being to use $<span class="math notranslate nohighlight">\(\varepsilon\)</span>$-greedy strategies,
leading to the following solution for the improvement task:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\sigma'(s) = 
\begin{cases}
\text{argmax}_{a \in A} q_{\sigma}(s,a) &amp; \text{ with probability } 1 - \varepsilon \\
\text{uniform distribution on actions} &amp; \text{ with probability } \varepsilon
\end{cases}
\end{split}\]</div>
<p>The fact that the strategy plays each action with positive probability implies that when evaluating $<span class="math notranslate nohighlight">\(\sigma'\)</span>$ we collect information about each action for every visited state.
Note that some states may never be visited, that’s fine.</p>
</div>
<div class="section" id="the-complete-algorithm">
<h2>The complete algorithm<a class="headerlink" href="#the-complete-algorithm" title="Permalink to this headline">¶</a></h2>
<p>The Monte Carlo policy iteration algorithm is:
starting from any strategy, apply alternatively evaluation and improvement to construct better and better strategies.
The theoretical guarantees are slim, since unless we evaluate strategies for infinitely many times, $<span class="math notranslate nohighlight">\(\widehat{\val}_\sigma\)</span><span class="math notranslate nohighlight">\( may not be equal (or even close!) to \)</span><span class="math notranslate nohighlight">\(\val_\sigma.\)</span>$
But in practice, it works.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./MonteCarlo"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="index.html" title="previous page">The Monte Carlo approach</a>
    <a class='right-next' id="next-link" href="../TwoPlayer/index.html" title="next page">Two-player games</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Nathanaël Fijalkow, Guillaume Lagarde, and Théo Matricon<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>