
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Proof of the UCB algorithm &#8212; Reinforcement learning</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"R": "\\mathbb{R}", "Q": "\\mathbb{Q}", "N": "\\mathbb{N}", "Z": "\\mathbb{Z}", "M": "\\mathcal{M}", "A": "\\mathcal{A}", "B": "\\mathcal{B}", "E": "\\mathbb{E}", "P": "\\mathbb{P}", "val": "\\text{val}", "Dist": "\\mathcal{D}"}}, "tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Markov decision processes" href="../MDP/index.html" />
    <link rel="prev" title="The model" href="intro.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Reinforcement learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Multi-armed bandits
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="index.html">
   Multi-armed bandits
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="intro.html">
     The model
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Proof of the UCB algorithm
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Markov decision processes and dynamic algorithms
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../MDP/index.html">
   Markov decision processes
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../MDP/intro.html">
     Model
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../DynamicAlgorithms/index.html">
   Dynamic algorithms
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../DynamicAlgorithms/intro.html">
     Dynamic algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../DynamicAlgorithms/finite_horizon.html">
     The finite horizon case
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../DynamicAlgorithms/discounted.html">
     The discounted reward
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../DynamicAlgorithms/Bellman_equations.html">
     Bellman equations: characterisation of the optimal values
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../DynamicAlgorithms/value_iteration.html">
     The value iteration algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../DynamicAlgorithms/policy_iteration.html">
     The policy iteration algorithm
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../DynamicAlgorithms/generalised_policy_iteration.html">
     The generalised policy iteration algorithm
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Monte Carlo approaches, temporal differences, and off-policy learning
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../MonteCarlo/index.html">
   The Monte Carlo approach
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../MonteCarlo/intro.html">
     Model
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Two-player games
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../TwoPlayer/index.html">
   Two-player games
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../TwoPlayer/intro.html">
     The model of two-player games
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../TwoPlayer/classic.html">
     Classical algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../TwoPlayer/MCTS.html">
     Monte Carlo Tree Search (MCTS) for two-player games
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/MAB/proof_UCB.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://nathanael-fijalkow.github.com/reinforcement_learning"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://nathanael-fijalkow.github.com/reinforcement_learning/issues/new?title=Issue%20on%20page%20%2FMAB/proof_UCB.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-class-of-algorithms-index-policies">
   A class of algorithms: index policies
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-the-choice-of-c-i-t">
   Why the choice of
   <span class="math notranslate nohighlight">
    \(c(i,t)\)
   </span>
   ?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proof-of-the-logarithmic-regret">
   Proof of the logarithmic regret
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="proof-of-the-ucb-algorithm">
<h1>Proof of the UCB algorithm<a class="headerlink" href="#proof-of-the-ucb-algorithm" title="Permalink to this headline">¶</a></h1>
<p>We present a proof that the upper confidence bound yields an (asymptotically) optimal algorithm for regret minimisation of multi-armed bandits.</p>
<p>There is an accompanying github repository for experimenting with different algorithms for the <a class="reference external" href="https://github.com/nathanael-fijalkow/Multi-Armed-Bandits">multi-armed bandits</a>.</p>
<p>This result was proved by Auer, Cesa-Bianchi, and Fischer in <a class="reference external" href="http://homes.di.unimi.it/cesa-bianchi/Pubblicazioni/ml-02.pdf">this paper</a>.
The proof is essentially the same (some calculations are a bit different).</p>
<div class="section" id="a-class-of-algorithms-index-policies">
<h2>A class of algorithms: index policies<a class="headerlink" href="#a-class-of-algorithms-index-policies" title="Permalink to this headline">¶</a></h2>
<p>A very natural class of algorithms is to assign at each time step <span class="math notranslate nohighlight">\(t\)</span> and to each machine <span class="math notranslate nohighlight">\(i\)</span> a number <span class="math notranslate nohighlight">\(U(i,t)\)</span>,
and to choose at each step the machine maximising this number.
(We first choose each machine once, to meaningfully initialise these numbers. We will not take this phase into account in the analysis later.)</p>
<p>The first naive attempt is to use <span class="math notranslate nohighlight">\(U(i,t) = \widehat{X}(i,t)\)</span>.
The empirical reward converges towards the actual expected reward, i.e. <span class="math notranslate nohighlight">\(\lim_T \widehat{X}(i,T) = \mu_i\)</span>,
but this does not imply that this algorithm finds the optimal machine in the limit.
Indeed, it may choose a machine once, get a bad reward from it, and decide never to play it again.
Consider for instance the case of two machines, the first one giving always reward <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span>, and the second giving reward <span class="math notranslate nohighlight">\(0\)</span> with probability <span class="math notranslate nohighlight">\(\frac{1}{4}\)</span>
and <span class="math notranslate nohighlight">\(1\)</span> with probability <span class="math notranslate nohighlight">\(\frac{3}{4}\)</span>. The second machine is optimal since <span class="math notranslate nohighlight">\(\mu_1 = \frac{1}{2}\)</span> and <span class="math notranslate nohighlight">\(\mu_2 = \frac{3}{4}\)</span>, but if we are unlucky and the first reward given by the machine <span class="math notranslate nohighlight">\(2\)</span> is <span class="math notranslate nohighlight">\(0\)</span>,
then the machine <span class="math notranslate nohighlight">\(2\)</span> will never be used again. This yields a regret which is linear in <span class="math notranslate nohighlight">\(T\)</span>.</p>
<p>The missing information is whether the empirical reward for machine <span class="math notranslate nohighlight">\(i\)</span> can be considered <strong>accurate or not</strong>, in other words whether the machine <span class="math notranslate nohighlight">\(i\)</span> was played enough times to trust the empirical reward.
This leads to indices of the form <span class="math notranslate nohighlight">\(U(i,t) = \widehat{X}(i,t) + c(i,t)\)</span>, with <span class="math notranslate nohighlight">\(c(i,t)\)</span> reflecting on the quality of the empirical reward.</p>
<p>Another point of view is that using <span class="math notranslate nohighlight">\(U(i,t) = \widehat{X}(i,t)\)</span> focusses on <strong>exploitation</strong> and does not reward <strong>exploration</strong>, which is precisely what <span class="math notranslate nohighlight">\(c(i,t)\)</span> does.</p>
<p>It turns out that there is a choice of <span class="math notranslate nohighlight">\(c(i,t)\)</span> which yields an algorithm with logarithmic regret:</p>
<blockquote>
<div><p><strong>Theorem:</strong> (Auer, Cesa-Bianchi, and Fischer 2002)
The regret of the index policy for <span class="math notranslate nohighlight">\(c(i,t) = \sqrt{ \frac{\log(t)}{n(i,t)} }\)</span> is <span class="math notranslate nohighlight">\(O(\log(T))\)</span></p>
</div></blockquote>
<p>This is asymptotically optimal as proved by Lai and Robbins in 1985.</p>
</div>
<div class="section" id="why-the-choice-of-c-i-t">
<h2>Why the choice of <span class="math notranslate nohighlight">\(c(i,t)\)</span>?<a class="headerlink" href="#why-the-choice-of-c-i-t" title="Permalink to this headline">¶</a></h2>
<p>The role of <span class="math notranslate nohighlight">\(c(i,t)\)</span> is to quantify the accuracy of the empirical reward.
Indeed, if the machine <span class="math notranslate nohighlight">\(i\)</span> was not played a lot of times, its emprical reward may be off by a lot,
and it is therefore interesting to choose it to increase the accuracy of the empirical reward, so <span class="math notranslate nohighlight">\(c(i,t)\)</span> should be large in that case.</p>
<p>The Chernoff-Hoeffding bound gives some information on the accuracy of the empirical reward:</p>
<div class="math notranslate nohighlight">
\[
\P(\widehat{X}(i,t) \ge \mu_i + c) \le \exp^{-2 c^2 n(i,t)}
\]</div>
<p>and similarly</p>
<div class="math notranslate nohighlight">
\[
\P(\widehat{X}(i,t) \le \mu_i - c) \le \exp^{-2 c^2 n(i,t)}
\]</div>
<p>So, choosing <span class="math notranslate nohighlight">\(c(i,t) = \sqrt{ \frac{\log(t)}{n(i,t)} }\)</span> implies an upper bound in <span class="math notranslate nohighlight">\(t^{-2}\)</span>, which we will see is exactly what we need.</p>
</div>
<div class="section" id="proof-of-the-logarithmic-regret">
<h2>Proof of the logarithmic regret<a class="headerlink" href="#proof-of-the-logarithmic-regret" title="Permalink to this headline">¶</a></h2>
<p>We first observe that the regret <span class="math notranslate nohighlight">\(R\)</span> is equal to</p>
<div class="math notranslate nohighlight">
\[
\sum_{i \in [1,K]} (\mu_* - \mu_i) \cdot n(i,T)
\]</div>
<p>Indeed, <span class="math notranslate nohighlight">\(\E[\sum_{t \in [1,T]} X_t] = \sum_{i \in [1,K]} \mu_i \cdot n(i,T)\)</span>.</p>
<p>This means that it will be enough to upper bound <span class="math notranslate nohighlight">\(n(i,T)\)</span> for all non-optimal machines <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>We let <span class="math notranslate nohighlight">\(\Delta_i\)</span> denote <span class="math notranslate nohighlight">\(\mu_* - \mu_i\)</span>.</p>
<p>We fix a non-optimal machine <span class="math notranslate nohighlight">\(i\)</span>.
Let <span class="math notranslate nohighlight">\(\ell\)</span> to be determined later: since when the machine <span class="math notranslate nohighlight">\(i\)</span> has been played only a few times, the empirical reward is very noisy,
we will be interested in studying the situation where it has been played at least <span class="math notranslate nohighlight">\(\ell\)</span> times.
For an event <span class="math notranslate nohighlight">\(A\)</span>, we write $<span class="math notranslate nohighlight">\(\{A\}\)</span><span class="math notranslate nohighlight">\( for the random variable with value \)</span>1<span class="math notranslate nohighlight">\( if \)</span>A<span class="math notranslate nohighlight">\( is realised and \)</span>0$ otherwise.</p>
<div class="math notranslate nohighlight">
\[
n(i,T) \le \ell + \sum_{t \in [1,T]} \{M(t) = i \wedge n(i,t) \ge \ell\}
\]</div>
<p>By definition of the algorithm, for the machine <span class="math notranslate nohighlight">\(i\)</span> to be chosen over the optimal machine <span class="math notranslate nohighlight">\(*\)</span>, we must have</p>
<div class="math notranslate nohighlight">
\[
\widehat{X}_{*,t} + c(*,t) \le \widehat{X}_{i,t} + c(i,t)
\]</div>
<p>Let us denote by <span class="math notranslate nohighlight">\(A\)</span> the event</p>
<div class="math notranslate nohighlight">
\[
\widehat{X}_{*,t} + c(*,t) \le \widehat{X}_{i,t} + c(i,t)\ \wedge\ n(i,t) \ge \ell
\]</div>
<p>Let us define two events:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(A_1 = \widehat{X}_{*,t} \le \mu_{*} - c(*,t)\)</span>, meaning that the optimal machine is underapproximated by <span class="math notranslate nohighlight">\(c(*,t)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(A_2 = \widehat{X}_{i,t} \ge \mu_i + c(i,t)\)</span>, meaning that <span class="math notranslate nohighlight">\(i\)</span> is overapproximated by <span class="math notranslate nohighlight">\(c(i,t)\)</span></p></li>
</ul>
<p>We let <span class="math notranslate nohighlight">\(\ell = \frac{4 \log(T)}{\Delta_i^2}\)</span>, and show that this implies that
<span class="math notranslate nohighlight">\(A \subseteq A_1 \vee A_2\)</span>.</p>
<p>First, we have</p>
<div class="math notranslate nohighlight">
\[
\mu_* \ge \mu_i + 2 c(i,t).
\]</div>
<p>Indeed, for <span class="math notranslate nohighlight">\(n(i,t) \ge \ell\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
2c(i,t) = \sqrt{\frac{4 \log(t)}{n(i,t)}} \le \Delta_i = \mu_* - \mu_i.
\]</div>
<p>We prove the contrapositive. Assume that neither <span class="math notranslate nohighlight">\(A_1\)</span> nor <span class="math notranslate nohighlight">\(A_2\)</span> are realised, then we have
<span class="math notranslate nohighlight">\(\widehat{X}_{*,t} + c(*,t) &gt; \mu_{*} \ge \mu_i + 2 c(i,t) &gt; \widehat{X}_{i,t} + c(i,t)\)</span>,
meaning that <span class="math notranslate nohighlight">\(A\)</span> is not realised.</p>
<p>It follows that the probability of <span class="math notranslate nohighlight">\(A\)</span> is bounded from above by the sum of the probabilities of the two events <span class="math notranslate nohighlight">\(A_1\)</span> and <span class="math notranslate nohighlight">\(A_2\)</span>.
The discussion above for the Chernoff-Hoeffding bound shows that they have each probability upper bounded by <span class="math notranslate nohighlight">\(t^{-2}\)</span>.</p>
<p>Thus</p>
<div class="math notranslate nohighlight">
\[
n(i,T) \le \ell + \sum_{t \ge 1} 2 t^{-2} \le \frac{4 \log(T)}{\Delta_i^2} + \frac{\pi^2}{3}
\]</div>
<p>We can conclude: the regret is bounded by</p>
<div class="math notranslate nohighlight">
\[
\left( \sum_{i \in [1,K]} \frac{4}{\Delta_i^2} \right) \cdot \log(T) + \frac{\pi^2}{3} = O(\log(T))
\]</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./MAB"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">The model</a>
    <a class='right-next' id="next-link" href="../MDP/index.html" title="next page">Markov decision processes</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Nathanaël Fijalkow, Guillaume Lagarde, and Théo Matricon<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>