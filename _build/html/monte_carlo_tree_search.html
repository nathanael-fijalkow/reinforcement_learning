
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Some classical algorithms &#8212; Reinforcement learning</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Reinforcement learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="content.html">
   Content in Jupyter Book
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="markdown.html">
     Markdown Files
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="notebooks.html">
     Content with notebooks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/monte_carlo_tree_search.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fmonte_carlo_tree_search.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Some classical algorithms
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#monte-carlo-search-tree-the-uct-algorithm">
   Monte Carlo search tree: the UCT algorithm
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      R: "{\\mathbb{R}}",
      Q: "{\\mathbb{Q}}",
      N: "{\\mathbb{N}}",
      Z: "{\\mathbb{Z}}",
      A: "{\\mathcal{A}}",
      B: "{\\mathcal{B}}",
      E: "{\\mathbb{E}}",
      P: "{\\mathbb{P}}",
      val: "{\\text{val}}",
    }
  }
});
</script>
<p class="intro"><span class="dropcap">W</span>e present the Monte Carlo tree search algorithm for finding an optimal strategy in a two-player game.</p>
<p>There is an accompanying github repository for implementations of some <a class="reference external" href="https://github.com/nathanael-fijalkow/TicTacToe_RL">algorithms solving Tic-Tac-Toe</a>.</p>
<p>The setting we consider is two-player (called Max and Min) games played on a tree, with perfect imperfect and turn-based moves.
Adding random moves or simulatenous moves (also called concurrent) would not make the question much more complicated (but at the price of heavier notations).
Considering multi-agents or imperfect information however is significantly harder and we will not touch upon these topics here.</p>
<p>One assumption we make here is that the outcome of a play <strong>can only be determined at the end of the play</strong>.
For win/lose games the outcome is 1 (win) or 0 (lose), but we can also consider more complicated (numerical) outcomes (in [0,1] for the sake of simplicity).
The fact that the quality of a play cannot be (a priori) evaluated along the way is a key difficulty we are facing here.</p>
<p>The goal is to find an optimal strategy.
Towards this goal we compute the function $<span class="math notranslate nohighlight">\(\val : N \to [0,1]\)</span><span class="math notranslate nohighlight">\( where \)</span><span class="math notranslate nohighlight">\(N\)</span>$ is the set of nodes of the tree,
defined by</p>
<div class="math notranslate nohighlight">
\[
\val(v) = \sup_{\sigma \text{ strategy of Max}} \inf_{\tau \text{ strategy of Min}} \text{Outcome}_v(\sigma,\tau)
\]</div>
<p>In words: <span class="math notranslate nohighlight">\(\val(v)\)</span> is the best outcome Max can secure against any strategy of Min starting from the node <span class="math notranslate nohighlight">\(v\)</span>.</p>
<div class="section" id="some-classical-algorithms">
<h1>Some classical algorithms<a class="headerlink" href="#some-classical-algorithms" title="Permalink to this headline">¶</a></h1>
<p>The very first algorithm is <strong>min max</strong>, it simply consists in unravelling the definition of $<span class="math notranslate nohighlight">\(\val\)</span>$:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\val(v) = 
\begin{cases}
\max_{v' \text{child of } v} \val(v') &amp; \text{if } v \text{ is controlled by Max} \\ 
\min_{v' \text{child of } v} \val(v') &amp; \text{if } v \text{ is controlled by Min}
\end{cases} 
\end{split}\]</div>
<p>Unfortunately, this algorithm is in practice way too computationally expensive, for two reasons: for most games,</p>
<ul class="simple">
<li><p>the <em>depth</em> of the tree (length of a play) is too large,</p></li>
<li><p>and most importantly, the <em>branching factor</em> (number of possible moves from a position) is too large.</p></li>
</ul>
<p>The first improvement is just to do lazy evaluations, sometimes called <strong>alpha beta pruning</strong>.
Concretely, if the outcomes are <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>: whenever some child of a Max node has value <span class="math notranslate nohighlight">\(1\)</span>, the value of the node becomes <span class="math notranslate nohighlight">\(1\)</span>, without further computation.
For outcomes in <span class="math notranslate nohighlight">\([0,1]\)</span>, each node is assigned two values, <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>, which are lower and upper bounds on the value of the node.
The current evaluation can be used to discard suboptimal moves.</p>
<p>Although this does lead to performance improvement in some cases, it is not enough to tackle most games.
An approach we do not discuss here is <strong>iterative deepening</strong>: we fix a depth and stop the search after this depth, evaluating the plays thus far.
This requires having an evaluation of partial plays, which may not be easy to find (for instance for Go).</p>
<p>The natural next idea is to add randomisation using a <strong>Monte Carlo</strong> approach: rather than enumerating for each node all of the children,
we randomly sample a subset of them.
The naive Monte Carlo approach performs (often) better than the deterministic algorithms, but the real game changer is yet to come.</p>
</div>
<div class="section" id="monte-carlo-search-tree-the-uct-algorithm">
<h1>Monte Carlo search tree: the UCT algorithm<a class="headerlink" href="#monte-carlo-search-tree-the-uct-algorithm" title="Permalink to this headline">¶</a></h1>
<p>There is a zoo of “Monte Carlo search tree algorithms”, and finding better and better instances is a very active research question nowadays.
Among them, the UCT (Upper Confidence bounds for Trees) is special because it was among the first one and it had a decisive impact on IA for Go playing.
It was introduced by Kocsis and Szepesvári in <a class="reference external" href="https://sites.ualberta.ca/~szepesva/papers/ecml06.pdf">this paper</a>.</p>
<p>The basic idea is simple: <strong>bias the sample procedure (choosing which children to explore) by the current estimates of their values</strong>.</p>
<p>Each node <span class="math notranslate nohighlight">\(v\)</span> induces an instance of the multi-armed bandit problem
(see this [blog post]() for some background on the problem and an analysis of the UCB algorithm):
each child is seen as a machine with some unknown distribution that can be evaluated.
The choice of the next machine to be evaluated is in the hands of a UCB algorithm,
which maintains for each machine $<span class="math notranslate nohighlight">\(v'\)</span><span class="math notranslate nohighlight">\( an interval \)</span><span class="math notranslate nohighlight">\(I(v',t)\)</span><span class="math notranslate nohighlight">\( (evolving with the time step \)</span>t<span class="math notranslate nohighlight">\(), and chooses the machine with the maximal (or minimal) value with respect to \)</span><span class="math notranslate nohighlight">\(I\)</span><span class="math notranslate nohighlight">\(.
More specifically, the definition of \)</span><span class="math notranslate nohighlight">\(I(v',t)\)</span><span class="math notranslate nohighlight">\( for a child \)</span>v’<span class="math notranslate nohighlight">\( of a node \)</span>v$ is</p>
<div class="math notranslate nohighlight">
\[
I(v',t) = [ \widehat{X}(v',t) - \sqrt{ \frac{\log(n(v,t))}{n(v',t)} }, \widehat{X}(v',t) + \sqrt{ \frac{\log(n(v,t))}{n(v',t)} } ]
\]</div>
<p>The term $<span class="math notranslate nohighlight">\(\widehat{X}(v',t)\)</span><span class="math notranslate nohighlight">\( is the empirical average outcome of the observed plays including \)</span>v’<span class="math notranslate nohighlight">\(, 
and the term \)</span><span class="math notranslate nohighlight">\(\sqrt{ \frac{\log(n(v,t))}{n(v',t)} }\)</span><span class="math notranslate nohighlight">\( quantifies the accuracy of the first term (in particular, it decreases with \)</span>n(v’,t)<span class="math notranslate nohighlight">\(, the number of times \)</span>v’<span class="math notranslate nohighlight">\( has been seen until time \)</span>t$).</p>
<p>The algorithm goes as follows.
We sample plays starting from the root. During the sampling from a node <span class="math notranslate nohighlight">\(v\)</span>, there are two cases:</p>
<ul class="simple">
<li><p><em>selection</em>: either all children of the node have already been explored (implying that there are values $<span class="math notranslate nohighlight">\(I(v',t)\)</span><span class="math notranslate nohighlight">\( for all children of \)</span>v<span class="math notranslate nohighlight">\(), 
then the child is chosen by optimising the optimising value of \)</span>I$ among children (the treatment is different for the two players: Max maximises the upper bound, while Min minimises the lower bound)</p></li>
<li><p><em>expansion and simulation</em>: or some child of the node has never been explored, and then a child is chosen at random among those</p></li>
</ul>
<p>Once a play has reached a terminal node, we <em>update</em> the values of all the nodes along the play.</p>
<p>This algorithm is one of the main ingredients in the AlphaGo and AlphaGo Zero projects to beat human players at Go.
Interestingly, its theoretical properties are not very good, for a simple reason:
the values of $<span class="math notranslate nohighlight">\(I\)</span>$ do not (provably) accurately reflect the actual values.
The original paper proves that the regret grows logarithmically with time (altough I could not find the original proof of this: only the short version of the paper seems to exist!).
However, a very nice and simple example due to Coquelin and Munos (see the paper <a class="reference external" href="https://arxiv.org/abs/1408.2028">here</a>) shows that the regret can be very large
because the algorithm may fail to detect the optimal path if they are to be found among worse and worse paths (and some claim this explains how Lee Seedol won one match against AlphaGo!).</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Nathanaël Fijalkow, Guillaume Lagarde, and Théo Matricon<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>