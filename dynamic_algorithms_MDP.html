
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Dynamic algorithms &#8212; Reinforcement learning</title>
    
  <link rel="stylesheet" href="_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Reinforcement learning</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="content.html">
   Content in Jupyter Book
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="markdown.html">
     Markdown Files
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="notebooks.html">
     Content with notebooks
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/dynamic_algorithms_MDP.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fdynamic_algorithms_MDP.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-finite-horizon-case">
   The finite horizon case
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-discounted-case">
   The discounted case
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#characterisation-of-the-optimal-values">
     Characterisation of the optimal values
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-value-iteration-algorithm">
     The value iteration algorithm
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-complete-algorithm">
       The complete algorithm
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-policy-iteration-algorithm">
     The policy iteration algorithm
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-evaluation-task">
       The evaluation task
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-improvement-task">
       The improvement task
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       The complete algorithm
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-generalised-policy-iteration-algorithm">
     The generalised policy iteration algorithm
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      R: "{\\mathbb{R}}",
      Q: "{\\mathbb{Q}}",
      N: "{\\mathbb{N}}",
      Z: "{\\mathbb{Z}}",
      M: "{\\mathcal{M}}",
      A: "{\\mathcal{A}}",
      B: "{\\mathcal{B}}",
      E: "{\\mathbb{E}}",
      P: "{\\mathbb{P}}",
      val: "{\\text{val}}",
      Dist: "{\\mathcal{D}}",
    }
  }
});
</script>
<p class="intro"><span class="dropcap">W</span>e present the model of MDPS, some fundamental properties and the two dynamic algorithms, value iteration and policy iteration.</p>
<p>A Markov decision process is given by a set of states <span class="math notranslate nohighlight">\(S\)</span>, a set of actions <span class="math notranslate nohighlight">\(A\)</span>, and a transition function</p>
<div class="math notranslate nohighlight">
\[
\Delta : S \times A \to \Dist(S \times \R)
\]</div>
<p>In words: from the state <span class="math notranslate nohighlight">\(s\)</span> and playing action <span class="math notranslate nohighlight">\(a\)</span>, the outcome is drawn according to the distribution <span class="math notranslate nohighlight">\(\Delta(s,a)\)</span>, it yields a reward <span class="math notranslate nohighlight">\(r\)</span> and leads to state <span class="math notranslate nohighlight">\(s'\)</span> with probability <span class="math notranslate nohighlight">\(\Delta(s,a)(s',r)\)</span>.
Sometimes, the next-state and reward distributions are given independently, and actually in many cases the reward is deterministic and not stochastic.</p>
<p>We also specify an initial state <span class="math notranslate nohighlight">\(s_0\)</span>.</p>
<p>The objective of Markov decision processes is to model decision-making for a controller in a stochastic environment with immediate rewards.
Indeed, at every step a reward is observed (although in many scenarios we will consider the reward will be <span class="math notranslate nohighlight">\(0\)</span> at each step except for the terminal step).</p>
<p>The distinctive properties of Markov decision processes is the <strong>Markov property</strong>: the distribution for the next state and the reward depend only on the <strong>current</strong> state and action, and not on the whole history. One commonly say “given the present, the future does not depend on the past”, although I’m not sure this formulation helps.</p>
<p>A <strong>play</strong>, or <strong>history</strong>, is a sequence $<span class="math notranslate nohighlight">\((s_0,a_0,r_0,s_1,a_1,r_1,\dots)\)</span><span class="math notranslate nohighlight">\(, where \)</span>s_0<span class="math notranslate nohighlight">\( is the first state, \)</span>a_0<span class="math notranslate nohighlight">\( the first action, \)</span>r_0$ the first reward, and so on.</p>
<p>The decisions are represented by <strong>strategies</strong> (also called <strong>policies</strong>). There are five types:</p>
<ul class="simple">
<li><p>(general) strategies are functions $<span class="math notranslate nohighlight">\(\sigma : (S \times A \times \R)^* \times S \to \Dist(A)\)</span>$: given a history and the current state, the strategy chooses a distribution on actions</p></li>
<li><p><strong>pure</strong> strategies (also called <strong>deterministic</strong>) choose not a distribution on actions but a single action: $<span class="math notranslate nohighlight">\(\sigma : (S \times A \times \R)^* \times S \to A\)</span></p></li>
<li><p><strong>Markovian</strong> strategies depend only on the current state and the time step: $<span class="math notranslate nohighlight">\(\sigma : \N \times S \to \Dist(A)\)</span>$</p></li>
<li><p><strong>stationary</strong> strategies (also called <strong>positional</strong> or <strong>memoryless</strong>) depend only on the current state: $<span class="math notranslate nohighlight">\(\sigma : S \to \Dist(A)\)</span>$</p></li>
<li><p><strong>pure stationary</strong> strategies: $<span class="math notranslate nohighlight">\(\sigma : S \to A\)</span>$.</p></li>
</ul>
<p>It turns out that in most cases the simplest strategies will be enough for our purposes (we will prove this in due course).</p>
<p>An MDP <span class="math notranslate nohighlight">\(\M\)</span> together with a strategy <span class="math notranslate nohighlight">\(\sigma\)</span> induce a Markov chain, and in particular a probability measure <span class="math notranslate nohighlight">\(\P_\sigma\)</span>, or <span class="math notranslate nohighlight">\(\P_{\sigma,s_0}\)</span> when specifying the initial state <span class="math notranslate nohighlight">\(s_0\)</span>.
(We will not go into the formal definitions of the Markov chain and the measure here.)
We let <span class="math notranslate nohighlight">\(S_i\)</span> denote the random variable denoting the state, <span class="math notranslate nohighlight">\(A_i\)</span> the action played, and <span class="math notranslate nohighlight">\(R_i\)</span> the reward obtained at the <span class="math notranslate nohighlight">\(i\)</span>-th step.</p>
<p>Given a strategy <span class="math notranslate nohighlight">\(\sigma\)</span> we write <span class="math notranslate nohighlight">\(\val_\sigma\)</span> for the value function defined by</p>
<div class="math notranslate nohighlight">
\[
\val_\sigma(s) = \E_{\sigma,s} [\sum_i R_i],
\]</div>
<p>meaning the total expected reward obtained while playing <span class="math notranslate nohighlight">\(\sigma\)</span> and starting from <span class="math notranslate nohighlight">\(s\)</span>.
The sum is a priori infinite, hence may not exist in general.
There are two different solutions to that:</p>
<ul class="simple">
<li><p>we fix some horizon and only look at paths of length the fixed horizon,</p></li>
<li><p>we introduce a discount factor <span class="math notranslate nohighlight">\(\gamma \in (0,1)\)</span> we consider the total expected discounted reward:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\E_\sigma [\sum_i \gamma^i R_i]
\]</div>
<p>Since $<span class="math notranslate nohighlight">\(\gamma\)</span><span class="math notranslate nohighlight">\( is less than \)</span><span class="math notranslate nohighlight">\(1\)</span>$, the rewards collected at the beginning are more important than the rewards in a more distant future.</p>
<p>The (algorithmic) goal in <strong>reinforcement learning</strong> is to find a strategy maximising the total expected reward:</p>
<div class="math notranslate nohighlight">
\[
\val_*(s_0) = \sup_{\sigma \text{ strategy}} \val_\sigma(s_0)
\]</div>
<div class="section" id="dynamic-algorithms">
<h1>Dynamic algorithms<a class="headerlink" href="#dynamic-algorithms" title="Permalink to this headline">¶</a></h1>
<p>Dynamic algorithms make two (unrealistic) assumptions:</p>
<ul class="simple">
<li><p>the MDP has a finite set of states and actions, which can be both tabularised</p></li>
<li><p>the transition function is known</p></li>
</ul>
<p>The first item means that in particular we can write programs including <em>for loops</em> on the set of states and actions,
in other words sweep over all possible states and actions.
Why making these assumptions if they are not realistic? Because the algorithms we will construct in this setting will be useful for the more general setting.</p>
<p>We start with a simple lemma saying that Markovian strategies are as powerful as general ones.
Recall that a Markovian strategy chooses a distribution on actions based only on the current state and the current time step,
i.e. $<span class="math notranslate nohighlight">\(\sigma' : \N \times S \to \Dist(A).\)</span>$</p>
<blockquote>
<div><p><strong>Lemma</strong> For any (general) strategy <span class="math notranslate nohighlight">\(\sigma\)</span>, there exists a Markovian strategy <span class="math notranslate nohighlight">\(\sigma'\)</span> such that for all time step <span class="math notranslate nohighlight">\(t\)</span> we have
$<span class="math notranslate nohighlight">\(
\P_{\sigma,s_0}(S_t = s, A_t = a) = \P_{\sigma',s_0}(S_t = s, A_t = a)
\)</span><span class="math notranslate nohighlight">\( 
implying that
\)</span><span class="math notranslate nohighlight">\(
\E_{\sigma,s_0}[R_t] = \E_{\sigma',s_0}[R_t]
\)</span>$</p>
</div></blockquote>
<p>The proof is simply to define <span class="math notranslate nohighlight">\(\sigma'\)</span> as follows: after <span class="math notranslate nohighlight">\(t\)</span> steps from the state <span class="math notranslate nohighlight">\(s\)</span>, play action <span class="math notranslate nohighlight">\(a\)</span> with probability
$<span class="math notranslate nohighlight">\(\P_{\sigma,s_0}(A_t = a \mid S_t = s).\)</span>$</p>
<div class="section" id="the-finite-horizon-case">
<h2>The finite horizon case<a class="headerlink" href="#the-finite-horizon-case" title="Permalink to this headline">¶</a></h2>
<p>We consider plays of length (exactly) $<span class="math notranslate nohighlight">\(T\)</span>$ a fixed finite value.</p>
<p>We let $<span class="math notranslate nohighlight">\(\val_*(t,s)\)</span><span class="math notranslate nohighlight">\( denote the optimal expected total reward from \)</span>s<span class="math notranslate nohighlight">\( in \)</span>t<span class="math notranslate nohighlight">\( moves.
The goal is to construct an optimal strategy \)</span><span class="math notranslate nohighlight">\(\sigma\)</span>$, i.e. maximising the expected total payoff:</p>
<div class="math notranslate nohighlight">
\[
\val_\sigma(T,s) = \E_\sigma[ \sum_{t = 1}^T R_t ] = \val_*(T,s)
\]</div>
<blockquote>
<div><p><strong>Lemma</strong> $<span class="math notranslate nohighlight">\(\val_*(0,s) = 0\)</span><span class="math notranslate nohighlight">\( and 
\)</span><span class="math notranslate nohighlight">\(
\val_*(t,s) = \max_{a \in A} \sum_{s' \in S, r \in R} \Delta(s,a)(s',r) (r + \val_*(t-1,s'))
\)</span>$</p>
</div></blockquote>
<p>To see that this equation is correct, we remark that the value function for a (Markovian) strategy $<span class="math notranslate nohighlight">\(\sigma\)</span>$ satisfies the recursive equation:</p>
<div class="math notranslate nohighlight">
\[
\val_\sigma(t,s) = \sum_{a \in A} \sigma(s)(a) \sum_{s' \in S,r \in \R} \Delta(s,a)(s',r) (r + \val_\sigma(t-1,s'))
\]</div>
<p>The key argument uses convexity.
(A very similar proof will be given below in more details in the discounted case.)</p>
<p>The lemma in particular implies that there are optimal deterministic Markovian strategies.
The dependence on the time step is necessary: closer to the time step <span class="math notranslate nohighlight">\(T\)</span>, it makes sense to play moves with higher immediate rewards but dire consequences,
while for a small <span class="math notranslate nohighlight">\(t\)</span>, moves should be selected to optimise not only the reward but also the states for their potential to yield higher rewards.</p>
<p>The equation can easily be turned into an algorithm (with pseudo-polynomial complexity), which computes $<span class="math notranslate nohighlight">\(\val_*(t,s)\)</span><span class="math notranslate nohighlight">\(
for each \)</span>s<span class="math notranslate nohighlight">\( for \)</span>t<span class="math notranslate nohighlight">\( from \)</span>0<span class="math notranslate nohighlight">\( to \)</span>T$.</p>
</div>
<div class="section" id="the-discounted-case">
<h2>The discounted case<a class="headerlink" href="#the-discounted-case" title="Permalink to this headline">¶</a></h2>
<p>For the remainder of this post we consider the discounted payoff defined by</p>
<div class="math notranslate nohighlight">
\[
\sum_{t \ge 1} \gamma^t R_t
\]</div>
<p>for a fixed discount factor $<span class="math notranslate nohighlight">\(\gamma\)</span><span class="math notranslate nohighlight">\( less than \)</span><span class="math notranslate nohighlight">\(1.\)</span>$</p>
<p>Recall that $<span class="math notranslate nohighlight">\(\val_*(s) = \sup_{\sigma \text{ strategy}} \val_\sigma(s).\)</span><span class="math notranslate nohighlight">\(
The goal is to construct an optimal strategy \)</span><span class="math notranslate nohighlight">\(\sigma,\)</span>$ i.e. maximising the expected discounted payoff:</p>
<div class="math notranslate nohighlight">
\[
\val_\sigma(s_0) = \E_{\sigma,s_0} [ \sum_{t \ge 1} \gamma^t R_t ] = \val_*(s_0)
\]</div>
<p>We will present two algorithms:</p>
<ul class="simple">
<li><p><strong>value iteration</strong></p></li>
<li><p><strong>policy iteration</strong> (sometimes called <strong>strategy iteration</strong> or <strong>strategy improvement</strong>)</p></li>
</ul>
<p>In the end we will argue that these algorithms both fall into a larger family of algorithms, called generalised policy iteration.</p>
<div class="section" id="characterisation-of-the-optimal-values">
<h3>Characterisation of the optimal values<a class="headerlink" href="#characterisation-of-the-optimal-values" title="Permalink to this headline">¶</a></h3>
<p>Let us introduce a convenient notation, the <span class="math notranslate nohighlight">\(q\)</span>-values.
For a strategy $<span class="math notranslate nohighlight">\(\sigma\)</span><span class="math notranslate nohighlight">\(, the \)</span>q$-value is</p>
<div class="math notranslate nohighlight">
\[
q_\sigma(s,a) = \sum_{s',r} \Delta(s,a)(s',r) (r + \gamma \val_\sigma(s'))
\]</div>
<p>We also denote $<span class="math notranslate nohighlight">\(q_*(s,a) = \sup_{\sigma \text{ strategy}} q_\sigma(s,a).\)</span>$</p>
<blockquote>
<div><p><strong>Lemma</strong> The optimal values satisfy the following equations
$<span class="math notranslate nohighlight">\(
\val_*(s) = \max_{a \in A} q_*(s,a)
\)</span>$</p>
</div></blockquote>
<p>We first show an inequality. Let <span class="math notranslate nohighlight">\(\sigma\)</span> denote an optimal strategy from <span class="math notranslate nohighlight">\(s\)</span>. Note that a priori, it may not be optimal from other states.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{llr}
\val_*(s) &amp; = &amp; \val_{\sigma}(s) \\
&amp; = &amp; \sum_{a \in A} \sigma(s)(a) q_\sigma(s,a) \\
&amp; \le &amp; \sum_{a \in A} \sigma(s)(a) q_*(s,a) \\
&amp; \le &amp; \max_{a \in A} q_*(s,a) &amp; \text{ by convexity}
\end{array}
\end{split}\]</div>
<p>To show the converse inequality, let <span class="math notranslate nohighlight">\(a\)</span> reaching the maximum in the term on the right hand side.
Let <span class="math notranslate nohighlight">\(\sigma_s\)</span> denote an optimal strategy from $<span class="math notranslate nohighlight">\(s,\)</span><span class="math notranslate nohighlight">\( for each \)</span><span class="math notranslate nohighlight">\(s.\)</span><span class="math notranslate nohighlight">\(
We define \)</span>\sigma<span class="math notranslate nohighlight">\( the strategy playing first \)</span>a<span class="math notranslate nohighlight">\(, and then simulating \)</span><span class="math notranslate nohighlight">\(\sigma_s\)</span><span class="math notranslate nohighlight">\( from \)</span><span class="math notranslate nohighlight">\(s\)</span><span class="math notranslate nohighlight">\( each each \)</span><span class="math notranslate nohighlight">\(s.\)</span>$
Then</p>
<div class="math notranslate nohighlight">
\[
\val_{\sigma}(s) = \sum_{s',r} \Delta(s,a)(s',r) (r + \gamma \val_{\sigma_{s'}}(s')) = \sum_{s',r} \Delta(s,a)(s',r) (r + \gamma \val_*(s')) = q_*(s,a)
\]</div>
<p>By definition $<span class="math notranslate nohighlight">\(\val_{\sigma}(s) \le \val_*(s)\)</span>$, so we proved the converse inequality.</p>
</div>
<div class="section" id="the-value-iteration-algorithm">
<h3>The value iteration algorithm<a class="headerlink" href="#the-value-iteration-algorithm" title="Permalink to this headline">¶</a></h3>
<p>We consider functions $<span class="math notranslate nohighlight">\(v : S \to \R\)</span><span class="math notranslate nohighlight">\( as vectors \)</span><span class="math notranslate nohighlight">\(v \in \R^S\)</span><span class="math notranslate nohighlight">\(, and write \)</span><span class="math notranslate nohighlight">\(|v|\)</span><span class="math notranslate nohighlight">\( for the infinity norm of \)</span><span class="math notranslate nohighlight">\(v\)</span><span class="math notranslate nohighlight">\(.
We extend the notation for the \)</span>q<span class="math notranslate nohighlight">\(-values to \)</span>v$, so</p>
<div class="math notranslate nohighlight">
\[
q_v(s,a) = \sum_{s',r} \Delta(s,a)(s',r) (r + \gamma v(s'))
\]</div>
<p>Let us denote by $<span class="math notranslate nohighlight">\(L\)</span><span class="math notranslate nohighlight">\( the operator \)</span><span class="math notranslate nohighlight">\(L : \R^S \to \R^S\)</span>$ defined by</p>
<div class="math notranslate nohighlight">
\[
L(v)(s) = \max_{a \in A} q_v(s,a).
\]</div>
<p>Now the lemma above reads:
The vector $<span class="math notranslate nohighlight">\(\val_*\)</span><span class="math notranslate nohighlight">\( is a fixed point of \)</span><span class="math notranslate nohighlight">\(L.\)</span>$</p>
<blockquote>
<div><p><strong>Lemma</strong> The operator $<span class="math notranslate nohighlight">\(L\)</span><span class="math notranslate nohighlight">\( is \)</span><span class="math notranslate nohighlight">\(\gamma\)</span><span class="math notranslate nohighlight">\(-Liptschitz, meaning for any \)</span><span class="math notranslate nohighlight">\(v,v'\)</span><span class="math notranslate nohighlight">\( we have
\)</span><span class="math notranslate nohighlight">\(
|L(v) - L(v')| \le \gamma |v - v'|.
\)</span>$</p>
</div></blockquote>
<p>It follows from Banach fixed point theorem (in the complete space $<span class="math notranslate nohighlight">\(\R^S\)</span><span class="math notranslate nohighlight">\( equipped with the infinity norm) 
that \)</span><span class="math notranslate nohighlight">\(L\)</span><span class="math notranslate nohighlight">\( has a unique fixed point and that it can be computed as follows:
let \)</span><span class="math notranslate nohighlight">\(v_0\)</span><span class="math notranslate nohighlight">\( an arbitrary vector, define \)</span><span class="math notranslate nohighlight">\(v_{n+1} = L(v_n)\)</span>$, then</p>
<div class="math notranslate nohighlight">
\[|v_{n+1} - v_n| \le \gamma^n |v_1 - v_0|\]</div>
<p>implying that $<span class="math notranslate nohighlight">\((v_n)_{n \in \N}\)</span><span class="math notranslate nohighlight">\( is a Cauchy sequence, hence converges to \)</span><span class="math notranslate nohighlight">\(v_\infty\)</span><span class="math notranslate nohighlight">\( such that \)</span><span class="math notranslate nohighlight">\(v_\infty = L(v_\infty)\)</span><span class="math notranslate nohighlight">\(,
and since \)</span><span class="math notranslate nohighlight">\(\val_*\)</span><span class="math notranslate nohighlight">\( is the unique fixed point of \)</span><span class="math notranslate nohighlight">\(L\)</span><span class="math notranslate nohighlight">\( we have \)</span><span class="math notranslate nohighlight">\(v_\infty = \val_*.\)</span>$</p>
<p>We also get an upper bound on the convergence rate:</p>
<div class="math notranslate nohighlight">
\[|\val_* - v_n| \le \frac{\gamma^n}{1 - \gamma} |v_1 - v_0|\]</div>
<div class="section" id="the-complete-algorithm">
<h4>The complete algorithm<a class="headerlink" href="#the-complete-algorithm" title="Permalink to this headline">¶</a></h4>
<p>We obtain an approximation algorithm: for a fixed $<span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span>$,</p>
<ul class="simple">
<li><p>By iterating the operator $<span class="math notranslate nohighlight">\(L\)</span><span class="math notranslate nohighlight">\( from any initial vector, compute \)</span><span class="math notranslate nohighlight">\(v\)</span><span class="math notranslate nohighlight">\( such that \)</span><span class="math notranslate nohighlight">\(\|L(v) - v\| \le \frac{\varepsilon}{2}\)</span><span class="math notranslate nohighlight">\(,
implying that \)</span><span class="math notranslate nohighlight">\(\|\val_* - v\| \le \varepsilon\)</span>$</p></li>
<li><p>Construct a pure positional strategy <span class="math notranslate nohighlight">\(\sigma\)</span>: define $<span class="math notranslate nohighlight">\(\sigma(s)\)</span><span class="math notranslate nohighlight">\( to be an action \)</span><span class="math notranslate nohighlight">\(a\)</span>$ such that</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
|v(s) - q_v(s,a)| \le \varepsilon.
\]</div>
<p>Then $<span class="math notranslate nohighlight">\(\sigma\)</span><span class="math notranslate nohighlight">\( is an \)</span><span class="math notranslate nohighlight">\(\varepsilon\)</span>$-optimal strategy, meaning that</p>
<div class="math notranslate nohighlight">
\[
\E_{\sigma,s_0} [ \sum_{t \ge 1} \gamma^t R_t ] \ge \val_*(s_0) - \varepsilon
\]</div>
<p>Thanks to the upper bound on the convergence rate, the number of iterations of the operator $<span class="math notranslate nohighlight">\(L\)</span><span class="math notranslate nohighlight">\( is \)</span><span class="math notranslate nohighlight">\(O(\log(\frac{1}{\varepsilon})).\)</span>$
Why is it called <strong>value iteration</strong>? Because it never considers any strategy, it only iterates on value functions.</p>
</div>
</div>
<div class="section" id="the-policy-iteration-algorithm">
<h3>The policy iteration algorithm<a class="headerlink" href="#the-policy-iteration-algorithm" title="Permalink to this headline">¶</a></h3>
<p>This second algorithm manipulates only pure positional strategies. We define two tasks:</p>
<ul class="simple">
<li><p><strong>evaluation</strong>: given a strategy $<span class="math notranslate nohighlight">\(\sigma\)</span><span class="math notranslate nohighlight">\(, compute (or approximate) \)</span><span class="math notranslate nohighlight">\(\val_\sigma\)</span>$</p></li>
<li><p><strong>improvement</strong>: given a strategy $<span class="math notranslate nohighlight">\(\sigma\)</span><span class="math notranslate nohighlight">\( and its (approximate) value function \)</span><span class="math notranslate nohighlight">\(\val_\sigma\)</span><span class="math notranslate nohighlight">\(, construct an improved strategy \)</span><span class="math notranslate nohighlight">\(\sigma'\)</span>$</p></li>
</ul>
<div class="section" id="the-evaluation-task">
<h4>The evaluation task<a class="headerlink" href="#the-evaluation-task" title="Permalink to this headline">¶</a></h4>
<p>Given a strategy $<span class="math notranslate nohighlight">\(\sigma\)</span>$, its values satisfy the following equations</p>
<div class="math notranslate nohighlight">
\[
\val_\sigma(s) = q_\sigma(s,\sigma(s)).
\]</div>
<p>One way of understanding this is that this forms a system of linear equations whose unknowns are $<span class="math notranslate nohighlight">\(\val_\sigma(s)\)</span><span class="math notranslate nohighlight">\( for each \)</span><span class="math notranslate nohighlight">\(s \in S\)</span>$.
This can be solved using Gaussian elimination, which theoretically works but would be very ineffective.</p>
<p>A more practical solution is to use the previous fixed point algorithm specialised to the strategy $<span class="math notranslate nohighlight">\(\sigma\)</span><span class="math notranslate nohighlight">\(:
we start from some vector \)</span><span class="math notranslate nohighlight">\(v_0\)</span>$ and then construct better and better approximations:</p>
<div class="math notranslate nohighlight">
\[
v_{n+1}(s) = q_{v_n}(s,\sigma(s)).
\]</div>
<p>For the same reasons as above, this process converges exponentially fast towards $<span class="math notranslate nohighlight">\(\val_\sigma\)</span><span class="math notranslate nohighlight">\(.
We fix a threshold \)</span><span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span><span class="math notranslate nohighlight">\( and stop when \)</span><span class="math notranslate nohighlight">\(|v_{n+1} - v_n| \le \frac{\varepsilon}{2},\)</span><span class="math notranslate nohighlight">\(
implying that \)</span><span class="math notranslate nohighlight">\(|\val_\sigma - v_n| \le \varepsilon.\)</span>$</p>
</div>
<div class="section" id="the-improvement-task">
<h4>The improvement task<a class="headerlink" href="#the-improvement-task" title="Permalink to this headline">¶</a></h4>
<p>Given a strategy $<span class="math notranslate nohighlight">\(\sigma\)</span><span class="math notranslate nohighlight">\( and its value function \)</span><span class="math notranslate nohighlight">\(\val_\sigma\)</span>$ (we ignore the fact that we actually only compute an approximation!),
we define</p>
<div class="math notranslate nohighlight">
\[
\sigma'(s) = \text{argmax}_{a \in A} q_{\sigma}(s,a)
\]</div>
<blockquote>
<div><p><strong>Lemma</strong> We have $<span class="math notranslate nohighlight">\(\val_{\sigma'} \ge \val_{\sigma}\)</span><span class="math notranslate nohighlight">\(.
Further, if for some \)</span><span class="math notranslate nohighlight">\(s\)</span><span class="math notranslate nohighlight">\( we have \)</span><span class="math notranslate nohighlight">\(q_{\sigma}(s,a) &gt; \val_{\sigma}(s)\)</span><span class="math notranslate nohighlight">\(,
then the inequality above is strict for \)</span><span class="math notranslate nohighlight">\(s.\)</span>$</p>
</div></blockquote>
<p>To see that the lemma holds, we write the equations for $<span class="math notranslate nohighlight">\(\val_\sigma\)</span><span class="math notranslate nohighlight">\( and \)</span><span class="math notranslate nohighlight">\(\val_{\sigma'}\)</span>$ and compare them.</p>
</div>
<div class="section" id="id1">
<h4>The complete algorithm<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>The policy iteration algorithm is:
starting from any strategy, apply alternatively evaluation and improvement to construct better and better strategies.
Note that since we consider only pure positional strategies, which are in finite number, and that each one strictly improves over the previous one,
then the algorithm terminates (that is, assuming we indeed compute the value function exactly, which we don’t!).
When it does so, the last strategy <span class="math notranslate nohighlight">\(\sigma\)</span> satisfies $<span class="math notranslate nohighlight">\(\val_\sigma(s) = \max_{a \in A} q_\sigma(s,a)\)</span>$,
which as we have seen above implies that it is optimal.</p>
<p>Why is it called <strong>policy iteration</strong>? Because it iterates on strategies (policies).</p>
</div>
</div>
<div class="section" id="the-generalised-policy-iteration-algorithm">
<h3>The generalised policy iteration algorithm<a class="headerlink" href="#the-generalised-policy-iteration-algorithm" title="Permalink to this headline">¶</a></h3>
<p>The generalised policy iteration algorithm simply alternates between evaluation and improvements steps, possibly not performing perfect evaluations.</p>
<p>Let us get back to the value iteration algorithm, and observe that in some sense it also consists in alternating evaluation and improvement steps.
At every step, we have a value function $<span class="math notranslate nohighlight">\(v_n\)</span><span class="math notranslate nohighlight">\( and the algorithm computes \)</span><span class="math notranslate nohighlight">\(v_{n+1} = L(v_n)\)</span>$ defined by</p>
<div class="math notranslate nohighlight">
\[
v_{n+1}(s) = \max_{a \in A} q_{v_n}(s,a).
\]</div>
<p>We can interpret this as doing the following two steps in one go:</p>
<ul class="simple">
<li><p>from $<span class="math notranslate nohighlight">\(v_n\)</span><span class="math notranslate nohighlight">\( compute the \)</span>q<span class="math notranslate nohighlight">\(-value \)</span><span class="math notranslate nohighlight">\(q_{v_n}(s,a)\)</span>$ through one step of the evaluation task</p></li>
<li><p>construct the value function $<span class="math notranslate nohighlight">\(v_{n+1}\)</span><span class="math notranslate nohighlight">\( induced by \)</span><span class="math notranslate nohighlight">\(q_{v_n}\)</span>$ through the improvement task</p></li>
</ul>
<p>The main difference with policy iteration is that $<span class="math notranslate nohighlight">\(v_n\)</span>$ does not represent the values of a given strategy.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Nathanaël Fijalkow, Guillaume Lagarde, and Théo Matricon<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>